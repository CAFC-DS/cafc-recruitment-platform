"""
Bulk Import Script for Archived Reports

This script imports multiple archived scout reports from Excel files.
- Only imports reports where player, fixture, and scout all match
- Skips failures and continues processing
- Generates detailed logs for successes and failures
- Batch processing for performance

Usage:
    python import_bulk_archived_reports.py
"""

import snowflake.connector
import os
import pandas as pd
from datetime import datetime
import re
from difflib import SequenceMatcher
from dotenv import load_dotenv
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
from pathlib import Path
import csv
import unicodedata
import json

# Load environment variables from .env file
load_dotenv()

# Snowflake connection parameters from environment
SNOWFLAKE_ACCOUNT = os.getenv("SNOWFLAKE_ACCOUNT")
SNOWFLAKE_USER = os.getenv("SNOWFLAKE_USERNAME")
SNOWFLAKE_WAREHOUSE = os.getenv("SNOWFLAKE_WAREHOUSE")
SNOWFLAKE_DATABASE = os.getenv("SNOWFLAKE_DATABASE")
SNOWFLAKE_SCHEMA = os.getenv("SNOWFLAKE_SCHEMA")
SNOWFLAKE_PRIVATE_KEY_PATH = os.getenv("SNOWFLAKE_PRIVATE_KEY_PATH")

# Excel file path
EXCEL_FILE = "/Users/hashim.umarji/Desktop/2025-26/Recruitment/Scout Reports/new-scout-reports-jan.xlsx"

# Batch size for inserts
BATCH_SIZE = 100

# Player mappings file (generated by parse_player_mappings.py)
PLAYER_MAPPINGS_FILE = "backend/player_mappings.json"

# Fuzzy matching performance configuration       # Maximum fixtures to check during fuzzy matching
FUZZY_THRESHOLD = 0.85             # Similarity threshold for fuzzy matching (85%)


def get_private_key():
    """Load private key from file for authentication."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    key_path = os.path.join(script_dir, SNOWFLAKE_PRIVATE_KEY_PATH)

    with open(key_path, "rb") as key:
        p_key = serialization.load_pem_private_key(
            key.read(),
            password=None,
            backend=default_backend()
        )

    return p_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption(),
    )


def get_snowflake_connection():
    """Create and return a Snowflake connection using private key authentication."""
    print("Connecting to Snowflake...")
    pkb = get_private_key()
    conn = snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        private_key=pkb
    )
    print("‚úì Connected to Snowflake\n")
    return conn

def load_player_mappings():
    """Load manual player name mappings from JSON file."""
    if not os.path.exists(PLAYER_MAPPINGS_FILE):
        print(f"‚ö†Ô∏è  Player mappings file not found: {PLAYER_MAPPINGS_FILE}")
        print("   Run parse_player_mappings.py first to generate it.\n")
        return {}

    print(f"Loading player name mappings from: {PLAYER_MAPPINGS_FILE}")
    with open(PLAYER_MAPPINGS_FILE, 'r', encoding='utf-8') as f:
        mappings = json.load(f)
    print(f"‚úì Loaded {len(mappings)} manual player name mappings\n")
    return mappings



def normalize_unicode(text):
    """Normalize unicode characters (remove accents, umlauts, etc.)."""
    if not text:
        return ""
    # NFD = Canonical Decomposition, separates base characters from diacritics
    nfd = unicodedata.normalize('NFD', text)
    # Keep only characters that are not combining diacritical marks
    return ''.join([c for c in nfd if unicodedata.category(c) != 'Mn'])




def normalize_team_name(team_name):
    """Normalize team name by removing common affixes, unicode, and extra spaces."""
    if not team_name:
        return []

    # First normalize unicode (√º ‚Üí u, √© ‚Üí e, etc.)
    team_name = normalize_unicode(team_name)

    # Remove extra spaces and convert to uppercase for comparison
    normalized = ' '.join(team_name.strip().upper().split())

    # Common prefixes to try removing (added SK for Slovak/Czech clubs)
    prefixes = ['FC ', 'AFC ', 'CF ', 'SC ', 'SK ', 'AC ', 'AS ', 'RC ', 'FK ']
    # Common suffixes to try removing
    suffixes = [' FC', ' AFC', ' CF', ' SC', ' SK', ' AC', ' AS', ' RC', ' FK',
                ' UNITED', ' CITY', ' TOWN', ' ATHLETIC', ' WANDERERS']

    # Create variations: original, without prefix, without suffix, without both
    variations = [normalized]

    for prefix in prefixes:
        if normalized.startswith(prefix):
            variations.append(normalized[len(prefix):])

    for suffix in suffixes:
        if normalized.endswith(suffix):
            variations.append(normalized[:-len(suffix)])

    return variations


def parse_fixture(fixture_str):
    """Parse fixture string in multiple formats."""
    if not fixture_str or pd.isna(fixture_str):
        return None, None

    fixture_str = str(fixture_str).strip()

    # Try format: "Team A 0-1 Team B" (dash score)
    match = re.match(r'^(.+?)\s+\d+-\d+\s+(.+)$', fixture_str)
    if match:
        return match.group(1).strip(), match.group(2).strip()

    # Try format: "Team A 0:1 Team B" (colon score)
    match = re.match(r'^(.+?)\s+\d+:\d+\s+(.+)$', fixture_str)
    if match:
        return match.group(1).strip(), match.group(2).strip()

    # Try format: "Team A - Team B" (no score)
    match = re.match(r'^(.+?)\s+-\s+(.+)$', fixture_str)
    if match:
        return match.group(1).strip(), match.group(2).strip()

    # Try format: "Team A vs Team B" or "Team A v Team B"
    match = re.match(r'^(.+?)\s+v[s]?\s+(.+)$', fixture_str, re.IGNORECASE)
    if match:
        return match.group(1).strip(), match.group(2).strip()

    return None, None

def find_player_with_mapping(cursor, player_name, all_players_cache, player_mappings):
    """Find player by name using manual mappings first, then fuzzy matching."""
    if not player_name or pd.isna(player_name):
        return None, "Empty player name"

    player_name = str(player_name).strip()

    # STEP 1: Check manual mappings FIRST
    if player_name in player_mappings:
        mapped_name = player_mappings[player_name]
        print(f"    Using manual mapping: '{player_name}' ‚Üí '{mapped_name}'")

        # Try exact match with mapped name (all_players_cache format: playerid, cafc_player_id, playername, squadname, position, data_source)
        for playerid, cafc_player_id, playername, squadname, position, data_source in all_players_cache:
            if playername.strip().upper() == mapped_name.strip().upper():
                return {
                    'playerid': playerid,
                    'cafc_player_id': cafc_player_id,
                    'player_name': playername,
                    'squad_name': squadname,
                    'position': position,
                    'data_source': data_source
                }, None

        # If exact match fails, try fuzzy with mapped name
        player_name = mapped_name  # Use mapped name for fuzzy matching

    # STEP 2: Try exact match (original or mapped name)
    for playerid, cafc_player_id, playername, squadname, position, data_source in all_players_cache:
        if playername.strip().upper() == player_name.strip().upper():
            return {
                'playerid': playerid,
                'cafc_player_id': cafc_player_id,
                'player_name': playername,
                'squad_name': squadname,
                'position': position,
                'data_source': data_source
            }, None

    # STEP 3: Try fuzzy matching (85% threshold)
    best_match = None
    best_score = 0.0
    normalized_search = normalize_unicode(player_name).upper().strip()

    for playerid, cafc_player_id, playername, squadname, position, data_source in all_players_cache:
        normalized_db = normalize_unicode(playername).upper().strip()
        score = SequenceMatcher(None, normalized_search, normalized_db).ratio()

        if score > best_score and score >= FUZZY_THRESHOLD:
            best_score = score
            best_match = {
                'playerid': playerid,
                'cafc_player_id': cafc_player_id,
                'player_name': playername,
                'squad_name': squadname,
                'position': position,
                'data_source': data_source,
                'score': score
            }

    if best_match:
        return best_match, None

    return None, f"Player not found: {player_name}"

def find_fixture_unlimited(cursor, fixture_str, fixture_date, fuzzy_log=None):
    """Find fixture with UNLIMITED fuzzy matching (no fixture count limits)."""
    if not fixture_str or pd.isna(fixture_str):
        return None, "Empty fixture"

    if not fixture_date or pd.isna(fixture_date):
        return None, "Empty fixture date"

    # Parse teams
    home_team, away_team = parse_fixture(fixture_str)
    if not home_team or not away_team:
        return None, f"Could not parse fixture: {fixture_str}"

    # Convert date
    try:
        if isinstance(fixture_date, str):
            date_obj = datetime.strptime(fixture_date, "%d/%m/%Y")
        else:
            date_obj = fixture_date
        formatted_date = date_obj.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        return None, f"Invalid date format: {fixture_date}"

    # PHASE 1: Try exact LIKE matching with team name variations
    home_variations = normalize_team_name(home_team)
    away_variations = normalize_team_name(away_team)

    for home_var in home_variations:
        for away_var in away_variations:
            query = """
                SELECT ID, CAFC_MATCH_ID, HOMESQUADNAME, AWAYSQUADNAME, DATA_SOURCE
                FROM MATCHES
                WHERE (
                    UPPER(HOMESQUADNAME) LIKE UPPER(%s) OR UPPER(HOMESQUADNAME) LIKE UPPER(%s)
                ) AND (
                    UPPER(AWAYSQUADNAME) LIKE UPPER(%s) OR UPPER(AWAYSQUADNAME) LIKE UPPER(%s)
                ) AND DATE(SCHEDULEDDATE) = %s
                LIMIT 1
            """

            home_like = f"%{home_var}%"
            away_like = f"%{away_var}%"

            cursor.execute(query, (home_like, away_like, away_like, home_like, formatted_date))
            result = cursor.fetchone()

            if result:
                match_id = result[1] if result[4] == 'internal' else result[0]
                return match_id, None

    # PHASE 2: Fuzzy matching with NO LIMITS
    query = """
        SELECT ID, CAFC_MATCH_ID, HOMESQUADNAME, AWAYSQUADNAME, DATA_SOURCE
        FROM MATCHES
        WHERE DATE(SCHEDULEDDATE) = %s
    """
    cursor.execute(query, (formatted_date,))
    all_fixtures = cursor.fetchall()

    if not all_fixtures:
        return None, f"No fixtures found on {formatted_date}"

    # Normalize search teams
    home_normalized = normalize_unicode(home_team).upper().strip()
    away_normalized = normalize_unicode(away_team).upper().strip()

    best_match = None
    best_score = 0.0

    for fixture in all_fixtures:
        db_home = normalize_unicode(fixture[2]).upper().strip()
        db_away = normalize_unicode(fixture[3]).upper().strip()

        # Calculate similarity scores (both directions)
        home_score = SequenceMatcher(None, home_normalized, db_home).ratio()
        away_score = SequenceMatcher(None, away_normalized, db_away).ratio()
        avg_score = (home_score + away_score) / 2

        # Also try swapped (home/away reversed)
        home_score_swap = SequenceMatcher(None, home_normalized, db_away).ratio()
        away_score_swap = SequenceMatcher(None, away_normalized, db_home).ratio()
        avg_score_swap = (home_score_swap + away_score_swap) / 2

        final_score = max(avg_score, avg_score_swap)

        if final_score > best_score and final_score >= FUZZY_THRESHOLD:
            best_score = final_score
            best_match = fixture

    if best_match:
        if fuzzy_log is not None:
            fuzzy_log.append({
                'search': f"{home_team} vs {away_team}",
                'matched': f"{best_match[2]} vs {best_match[3]}",
                'date': formatted_date,
                'similarity': f"{best_score:.2%}",
                'fixture_count': len(all_fixtures)
            })
        match_id = best_match[1] if best_match[4] == 'internal' else best_match[0]
        return match_id, None

    return None, f"Fixture not found: {fixture_str} on {formatted_date} ({len(all_fixtures)} fixtures checked)"


def find_scout(cursor, scout_name, all_scouts_cache):
    """Find scout by name and return user ID."""
    if not scout_name or pd.isna(scout_name):
        return None, "Empty scout name"

    scout_name = str(scout_name).strip()

    # Try matching by FIRSTNAME + LASTNAME
    for user in all_scouts_cache:
        full_name = f"{user[2]} {user[3]}".strip().upper()
        if full_name == scout_name.upper():
            return user[0], None

    # Try matching by USERNAME
    for user in all_scouts_cache:
        if user[1].upper() == scout_name.upper():
            return user[0], None

    return None, f"Scout not found: {scout_name}"


def create_scout_if_not_exists(conn, cursor, scout_name, all_scouts_cache):
    """Find scout by name, or create new user if not found. Returns user_id."""
    if not scout_name or pd.isna(scout_name):
        return None, "Empty scout name"

    scout_name = str(scout_name).strip()

    # Try to find existing scout
    user_id, _ = find_scout(cursor, scout_name, all_scouts_cache)
    if user_id is not None:
        return user_id, None

    # Scout not found - create new user
    print(f"    Creating new scout user: {scout_name}")

    try:
        # Parse name into first and last name (simple split on space)
        name_parts = scout_name.split()
        if len(name_parts) >= 2:
            first_name = name_parts[0]
            last_name = " ".join(name_parts[1:])
        else:
            first_name = scout_name
            last_name = ""

        # Generate username from name (lowercase, no spaces)
        username = scout_name.replace(" ", "").lower()

        # Check if username already exists
        cursor.execute("SELECT ID FROM USERS WHERE UPPER(USERNAME) = UPPER(%s)", (username,))
        if cursor.fetchone():
            # Username exists, add a number
            counter = 1
            while True:
                new_username = f"{username}{counter}"
                cursor.execute("SELECT ID FROM USERS WHERE UPPER(USERNAME) = UPPER(%s)", (new_username,))
                if not cursor.fetchone():
                    username = new_username
                    break
                counter += 1
                if counter > 100:  # Safety limit
                    return None, f"Could not generate unique username for: {scout_name}"

        # Create new user with Scout role
        # Note: Setting a default password hash (they'll need to reset it)
        default_password_hash = "$2b$12$LQv3c1yqBWVHxkd0LHAkCOYz6TtxMQJqhN8/LewY5GyYqDqWEKjSu"  # "changeme"

        insert_query = """
            INSERT INTO USERS (USERNAME, HASHED_PASSWORD, FIRSTNAME, LASTNAME, ROLE, EMAIL, CREATED_AT)
            VALUES (%s, %s, %s, %s, %s, %s, %s)
        """

        cursor.execute(insert_query, (
            username,
            default_password_hash,
            first_name,
            last_name,
            "Scout",
            f"{username}@archived.scout",  # Placeholder email
            datetime.now()
        ))

        # IMPORTANT: Commit immediately so the user is available for subsequent lookups
        conn.commit()

        # Get the new user ID
        cursor.execute("SELECT ID FROM USERS WHERE USERNAME = %s", (username,))
        result = cursor.fetchone()

        if result:
            new_user_id = result[0]
            # Add to cache for future lookups
            all_scouts_cache.append((new_user_id, username, first_name, last_name))
            print(f"    ‚úì Created scout user: {scout_name} (username: {username}, ID: {new_user_id})")
            return new_user_id, None
        else:
            return None, f"Failed to retrieve created user: {scout_name}"

    except Exception as e:
        print(f"    ‚úó Error creating scout user '{scout_name}': {str(e)}")
        # Try to rollback if something went wrong
        try:
            conn.rollback()
        except:
            pass
        return None, f"Error creating scout: {str(e)}"


def combine_content(strengths, weaknesses, summary, vss_score):
    """Combine strengths, weaknesses, summary, and VSS score into formatted text."""
    parts = []

    if strengths and not pd.isna(strengths) and str(strengths).strip():
        parts.append(f"STRENGTHS:\n{str(strengths).strip()}")

    if weaknesses and not pd.isna(weaknesses) and str(weaknesses).strip():
        parts.append(f"WEAKNESSES:\n{str(weaknesses).strip()}")

    if summary and not pd.isna(summary) and str(summary).strip():
        parts.append(f"SUMMARY:\n{str(summary).strip()}")

    if vss_score and not pd.isna(vss_score) and str(vss_score).strip():
        parts.append(f"VSS SCORE:\n{str(vss_score).strip()}")

    return "\n\n".join(parts)


def create_flag_report(cursor, player, match_id, user_id, combined_summary,
                       flag_category, scouting_type, report_date, position=""):
    """Create Flag report in database using dual ID system."""
    # Clean NaN values - convert to None or empty string
    if pd.isna(position) or str(position).lower() == 'nan':
        position = ""
    if pd.isna(scouting_type) or str(scouting_type).lower() == 'nan':
        scouting_type = "Video"
    if pd.isna(flag_category) or str(flag_category).lower() == 'nan':
        flag_category = ""

    # Determine which player ID column to use based on data source
    if player['data_source'] == 'internal':
        player_id = None
        cafc_player_id = player['cafc_player_id']
    else:
        player_id = player['playerid']
        cafc_player_id = None

    # Convert report date to datetime if it's a string
    if isinstance(report_date, str):
        report_date = datetime.strptime(report_date, "%d/%m/%Y")

    # Check for duplicate report
    # Convert report_date to string for Snowflake DATE comparison
    report_date_str = report_date.strftime("%Y-%m-%d") if isinstance(report_date, datetime) else report_date

    duplicate_check = """
        SELECT ID FROM SCOUT_REPORTS
        WHERE USER_ID = %s
        AND MATCH_ID = %s
        AND IS_ARCHIVED = TRUE
        AND (
            (PLAYER_ID = %s AND PLAYER_ID IS NOT NULL) OR
            (CAFC_PLAYER_ID = %s AND CAFC_PLAYER_ID IS NOT NULL)
        )
        AND TO_DATE(CREATED_AT) = TO_DATE(%s)
        LIMIT 1
    """

    cursor.execute(duplicate_check, (
        user_id,
        match_id,
        player_id,
        cafc_player_id,
        report_date_str
    ))

    if cursor.fetchone():
        # Duplicate found - skip insert
        return False

    query = """
        INSERT INTO SCOUT_REPORTS (
            USER_ID,
            PLAYER_ID,
            CAFC_PLAYER_ID,
            MATCH_ID,
            REPORT_TYPE,
            POSITION,
            FORMATION,
            BUILD,
            HEIGHT,
            SCOUTING_TYPE,
            SUMMARY,
            FLAG_CATEGORY,
            CREATED_AT,
            IS_ARCHIVED
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
        )
    """

    cursor.execute(query, (
        user_id,
        player_id,
        cafc_player_id,
        match_id,
        "Flag",
        position or "",
        "",  # formation - blank
        "",  # build - blank
        "",  # height - blank
        scouting_type or "Video",  # default to Video if not specified
        combined_summary,
        flag_category,
        report_date_str,  # Use string format for Snowflake
        True,  # IS_ARCHIVED
    ))

    return True  # Successfully created


def read_excel_files():
    """Read Master Scout Reports Excel file."""
    print("="*80)
    print("READING MASTER SCOUT REPORTS")
    print("="*80 + "\n")

    excel_file = Path(EXCEL_FILE)

    if not excel_file.exists():
        print(f"‚úó File not found: {EXCEL_FILE}")
        return None

    print(f"Reading: {excel_file.name}")
    print(f"Size: {excel_file.stat().st_size / 1024:.1f} KB\n")

    try:
        # Read Excel file - check available sheets
        xls = pd.ExcelFile(excel_file)
        print(f"Sheets available: {xls.sheet_names}\n")

        # Use first sheet by default (or 'Reports' if it exists)
        sheet_name = 'Reports' if 'Reports' in xls.sheet_names else 0
        print(f"Reading sheet: {sheet_name if isinstance(sheet_name, str) else xls.sheet_names[0]}")

        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        print(f"‚úì Loaded {len(df)} rows\n")

        # Add source file to each report
        df['source_file'] = excel_file.name

        return df

    except Exception as e:
        print(f"‚úó Error reading file: {str(e)}")
        import traceback
        traceback.print_exc()
        return None


def detect_columns(df):
    """Detect column names from DataFrame."""
    col_map = {}
    for col in df.columns:
        col_lower = col.lower().strip()
        # More flexible matching - check for exact match OR pattern match
        if col_lower == 'player' or ('player' in col_lower and 'name' in col_lower):
            col_map['player'] = col
        elif col_lower == 'fixture' or (col_lower == 'match' and 'id' not in col_lower):
            if 'fixture' not in col_map:  # Don't overwrite
                col_map['fixture'] = col
        elif col_lower == 'scout' or col_lower == 'author':
            col_map['scout'] = col
        elif col_lower == 'report date' or ('report' in col_lower and 'date' in col_lower and 'fixture' not in col_lower):
            col_map['report_date'] = col
        elif col_lower == 'fixture date' or (('fixture' in col_lower or 'match' in col_lower) and 'date' in col_lower):
            col_map['fixture_date'] = col
        elif col_lower == 'grade' or ('grade' in col_lower and 'traffic' not in col_lower):
            col_map['grade'] = col
        elif col_lower == 'strengths' or (col_lower.startswith('strength') and '(' in col_lower):
            if 'strengths' not in col_map:
                col_map['strengths'] = col
        elif col_lower == 'weaknesses' or (col_lower.startswith('weakness') and '(' in col_lower):
            if 'weaknesses' not in col_map:
                col_map['weaknesses'] = col
        elif col_lower == 'summary' or ('summary' in col_lower and '(' in col_lower):
            if 'summary' not in col_map:
                col_map['summary'] = col
        elif 'vss' in col_lower:
            col_map['vss'] = col
        elif col_lower == 'position' or 'position' in col_lower:
            if 'position' not in col_map:
                col_map['position'] = col
        elif col_lower == 'live/video' or ('live' in col_lower and 'video' in col_lower):
            col_map['scouting_type'] = col

    return col_map


def import_reports(conn, reports_df, player_mappings):
    """Import reports in batches."""
    print("="*80)
    print("IMPORTING REPORTS")
    print("="*80 + "\n")

    cursor = conn.cursor()

    # Cache all players and scouts
    print("Caching players...")
    cursor.execute("SELECT PLAYERID, CAFC_PLAYER_ID, PLAYERNAME, SQUADNAME, POSITION, DATA_SOURCE FROM PLAYERS")
    all_players = cursor.fetchall()
    print(f"‚úì Cached {len(all_players)} players")

    print("Caching users...")
    cursor.execute("SELECT ID, USERNAME, FIRSTNAME, LASTNAME FROM USERS")
    all_scouts = cursor.fetchall()
    print(f"‚úì Cached {len(all_scouts)} users\n")

    # Detect columns
    col_map = detect_columns(reports_df)
    print(f"Detected columns: {col_map}\n")

    if not all(k in col_map for k in ['player', 'fixture', 'scout']):
        print("‚úó ERROR: Missing required columns")
        return

    # Prepare CSV writers
    success_file = open('imported_reports.csv', 'w', newline='', encoding='utf-8')
    failure_file = open('failed_reports.csv', 'w', newline='', encoding='utf-8')

    success_writer = csv.writer(success_file)
    failure_writer = csv.writer(failure_file)

    # Write headers
    success_writer.writerow(['Row', 'Player', 'Fixture', 'Scout', 'Grade', 'Source File'])
    failure_writer.writerow(['Row', 'Player', 'Fixture', 'Scout', 'Error', 'Source File'])

    success_count = 0
    failure_count = 0
    batch_count = 0
    fuzzy_matches = []  # Track fuzzy fixture matches

    print(f"Processing {len(reports_df)} reports...\n")

    for idx, row in reports_df.iterrows():
        row_num = idx + 1

        if row_num % 10 == 0:
            percent = (row_num / len(reports_df)) * 100
            print(f"  Progress: {row_num}/{len(reports_df)} ({percent:.1f}%) - Success: {success_count}, Failed: {failure_count}")

        try:
            # Get values
            player_name = row.get(col_map.get('player'))
            fixture_str = row.get(col_map.get('fixture'))
            scout_name = row.get(col_map.get('scout'))
            fixture_date = row.get(col_map.get('fixture_date'))
            report_date = row.get(col_map.get('report_date'))
            grade = row.get(col_map.get('grade'))
            position = row.get(col_map.get('position', ''))
            scouting_type = row.get(col_map.get('scouting_type', 'Video'))
            source_file = row.get('source_file', 'Unknown')

            # Find player
            player, error = find_player_with_mapping(cursor, player_name, all_players, player_mappings)
            if error:
                failure_writer.writerow([row_num, player_name, fixture_str, scout_name, error, source_file])
                failure_count += 1
                continue

            # Find fixture
            match_id, error = find_fixture_unlimited(cursor, fixture_str, fixture_date, fuzzy_matches)
            if error:
                failure_writer.writerow([row_num, player_name, fixture_str, scout_name, error, source_file])
                failure_count += 1
                continue

            # Find or create scout
            user_id, error = create_scout_if_not_exists(conn, cursor, scout_name, all_scouts)
            if error:
                failure_writer.writerow([row_num, player_name, fixture_str, scout_name, error, source_file])
                failure_count += 1
                continue

            # Combine content
            combined_summary = combine_content(
                row.get(col_map.get('strengths')),
                row.get(col_map.get('weaknesses')),
                row.get(col_map.get('summary')),
                row.get(col_map.get('vss'))
            )

            # Create report (returns False if duplicate)
            was_created = create_flag_report(
                cursor,
                player,
                match_id,
                user_id,
                combined_summary,
                grade,
                scouting_type,
                report_date,
                position
            )

            if was_created:
                success_writer.writerow([row_num, player_name, fixture_str, scout_name, grade, source_file])
                success_count += 1
                batch_count += 1
            else:
                # Duplicate - skip it
                print(f"  ‚äò Row {row_num}: Skipped duplicate report for {player_name}")
                continue

            # Commit in batches
            if batch_count >= BATCH_SIZE:
                conn.commit()
                print(f"  ‚úì Committed batch of {batch_count} reports")
                batch_count = 0

        except Exception as e:
            error_msg = str(e)
            failure_writer.writerow([row_num, player_name, fixture_str, scout_name, error_msg, source_file])
            failure_count += 1
            continue

    # Final commit
    if batch_count > 0:
        conn.commit()
        print(f"  ‚úì Committed final batch of {batch_count} reports")

    # Close files
    success_file.close()
    failure_file.close()

    # Save fuzzy matches log
    if fuzzy_matches:
        with open('fuzzy_fixture_matches_import.txt', 'w') as f:
            f.write("FUZZY FIXTURE MATCHES (DURING IMPORT)\n")
            f.write("="*80 + "\n\n")
            f.write("These fixtures were matched using fuzzy string matching (not exact matches).\n")
            f.write("Similarity threshold: 85%\n\n")
            f.write("="*80 + "\n\n")
            for match in fuzzy_matches:
                f.write(f"SEARCHED FOR: {match['search']}\n")
                f.write(f"MATCHED WITH: {match['matched']}\n")
                f.write(f"DATE: {match['date']}\n")
                f.write(f"SIMILARITY: {match['similarity']}\n")
                f.write("-" * 80 + "\n\n")
        print(f"  ‚úì Saved fuzzy_fixture_matches_import.txt ({len(fuzzy_matches)} fuzzy matches)\n")

    cursor.close()

    # Print summary
    print("\n" + "="*80)
    print("IMPORT COMPLETE")
    print("="*80 + "\n")
    print(f"‚úÖ Successfully imported: {success_count} reports")
    print(f"‚ùå Failed: {failure_count} reports")
    print(f"\nüìÑ Results saved to:")
    print(f"  - imported_reports.csv")
    print(f"  - failed_reports.csv")
    if fuzzy_matches:
        print(f"  - fuzzy_fixture_matches_import.txt ({len(fuzzy_matches)} fuzzy matches)")


def main():
    """Main function to import bulk archived reports."""
    try:
        # Load player mappings
        player_mappings = load_player_mappings()

        # Read Excel files
        reports_df = read_excel_files()
        if reports_df is None or len(reports_df) == 0:
            print("‚úó No reports to import")
            return

        # Connect to database
        conn = get_snowflake_connection()

        # Import reports
        import_reports(conn, reports_df, player_mappings)

        # Close connection
        conn.close()

        print("\n‚úì All done!")

    except Exception as e:
        print(f"\n‚úó ERROR: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
