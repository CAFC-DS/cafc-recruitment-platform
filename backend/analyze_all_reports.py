"""
Unified comprehensive analysis script for archived scout reports.

This script:
1. Uses manual player name mappings from player_mappings.json
2. Uses UNLIMITED fuzzy matching for fixtures (no 100-fixture limit)
3. Provides comprehensive analysis of what can/cannot be imported
4. Does NOT import to database - analysis only

Run this BEFORE importing to understand exactly what will succeed/fail.
"""

import snowflake.connector
import pandas as pd
import os
from datetime import datetime
import re
from difflib import SequenceMatcher
from dotenv import load_dotenv
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
from pathlib import Path
import json
import unicodedata

load_dotenv()

# Snowflake connection parameters
SNOWFLAKE_ACCOUNT = os.getenv("SNOWFLAKE_ACCOUNT")
SNOWFLAKE_USER = os.getenv("SNOWFLAKE_USERNAME")
SNOWFLAKE_WAREHOUSE = os.getenv("SNOWFLAKE_WAREHOUSE")
SNOWFLAKE_DATABASE = os.getenv("SNOWFLAKE_DATABASE")
SNOWFLAKE_SCHEMA = os.getenv("SNOWFLAKE_SCHEMA")
SNOWFLAKE_PRIVATE_KEY_PATH = os.getenv("SNOWFLAKE_PRIVATE_KEY_PATH")

# Excel file path
EXCEL_FILE = "/Users/hashim.umarji/Downloads/ScoutReportsNew.xlsx"

# Player mappings file (generated by parse_player_mappings.py)
PLAYER_MAPPINGS_FILE = "backend/player_mappings.json"

# Fuzzy matching configuration
FUZZY_THRESHOLD = 0.85  # 85% similarity threshold


def get_private_key():
    """Load private key from file for authentication."""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    key_path = os.path.join(script_dir, SNOWFLAKE_PRIVATE_KEY_PATH)
    with open(key_path, "rb") as key:
        p_key = serialization.load_pem_private_key(
            key.read(),
            password=None,
            backend=default_backend()
        )
    return p_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption(),
    )


def get_snowflake_connection():
    """Create and return a Snowflake connection."""
    print("Connecting to Snowflake...")
    pkb = get_private_key()
    conn = snowflake.connector.connect(
        account=SNOWFLAKE_ACCOUNT,
        user=SNOWFLAKE_USER,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        private_key=pkb
    )
    print("‚úì Connected to Snowflake\n")
    return conn


def load_player_mappings():
    """Load manual player name mappings from JSON file."""
    if not os.path.exists(PLAYER_MAPPINGS_FILE):
        print(f"‚ö†Ô∏è  Player mappings file not found: {PLAYER_MAPPINGS_FILE}")
        print("   Run parse_player_mappings.py first to generate it.\n")
        return {}

    print(f"Loading player name mappings from: {PLAYER_MAPPINGS_FILE}")
    with open(PLAYER_MAPPINGS_FILE, 'r', encoding='utf-8') as f:
        mappings = json.load(f)
    print(f"‚úì Loaded {len(mappings)} manual player name mappings\n")
    return mappings


def normalize_unicode(text):
    """Normalize unicode characters (remove accents, umlauts, etc.)."""
    if not text:
        return ""
    nfd = unicodedata.normalize('NFD', text)
    return ''.join([c for c in nfd if unicodedata.category(c) != 'Mn'])


def normalize_team_name(team_name):
    """Normalize team name by removing common affixes, unicode, and extra spaces."""
    if not team_name:
        return []

    team_name = normalize_unicode(team_name)
    normalized = ' '.join(team_name.strip().upper().split())

    prefixes = ['FC ', 'AFC ', 'CF ', 'SC ', 'SK ', 'AC ', 'AS ', 'RC ', 'FK ']
    suffixes = [' FC', ' AFC', ' CF', ' SC', ' SK', ' AC', ' AS', ' RC', ' FK',
                ' UNITED', ' CITY', ' TOWN', ' ATHLETIC', ' WANDERERS']

    variations = [normalized]

    for prefix in prefixes:
        if normalized.startswith(prefix):
            variations.append(normalized[len(prefix):])

    for suffix in suffixes:
        if normalized.endswith(suffix):
            variations.append(normalized[:-len(suffix)])

    return variations


def parse_fixture(fixture_str):
    """Parse fixture string in multiple formats."""
    if not fixture_str or pd.isna(fixture_str):
        return None, None

    fixture_str = str(fixture_str).strip()

    patterns = [
        r'^(.+?)\s+\d+-\d+\s+(.+)$',  # "Team A 0-1 Team B"
        r'^(.+?)\s+\d+:\d+\s+(.+)$',  # "Team A 0:1 Team B"
        r'^(.+?)\s+-\s+(.+)$',         # "Team A - Team B"
        r'^(.+?)\s+v[s]?\s+(.+)$'      # "Team A vs Team B"
    ]

    for pattern in patterns:
        match = re.match(pattern, fixture_str, re.IGNORECASE)
        if match:
            return match.group(1).strip(), match.group(2).strip()

    return None, None


def find_player_with_mapping(cursor, player_name, all_players_cache, player_mappings):
    """Find player by name using manual mappings first, then fuzzy matching."""
    if not player_name or pd.isna(player_name):
        return None, "Empty player name"

    player_name = str(player_name).strip()

    # STEP 1: Check manual mappings FIRST
    if player_name in player_mappings:
        mapped_name = player_mappings[player_name]
        print(f"    Using manual mapping: '{player_name}' ‚Üí '{mapped_name}'")

        # Try exact match with mapped name
        for playerid, cafc_player_id, playername, data_source in all_players_cache:
            if playername.strip().upper() == mapped_name.strip().upper():
                if data_source == 'internal':
                    return {'CAFC_PLAYER_ID': cafc_player_id}, None
                else:
                    return {'PLAYERID': playerid}, None

        # If exact match fails, try fuzzy with mapped name
        player_name = mapped_name  # Use mapped name for fuzzy matching

    # STEP 2: Try exact match (original or mapped name)
    for playerid, cafc_player_id, playername, data_source in all_players_cache:
        if playername.strip().upper() == player_name.strip().upper():
            if data_source == 'internal':
                return {'CAFC_PLAYER_ID': cafc_player_id}, None
            else:
                return {'PLAYERID': playerid}, None

    # STEP 3: Try fuzzy matching (85% threshold)
    best_match = None
    best_score = 0.0
    normalized_search = normalize_unicode(player_name).upper().strip()

    for playerid, cafc_player_id, playername, data_source in all_players_cache:
        normalized_db = normalize_unicode(playername).upper().strip()
        score = SequenceMatcher(None, normalized_search, normalized_db).ratio()

        if score > best_score and score >= FUZZY_THRESHOLD:
            best_score = score
            best_match = {
                'playerid': playerid,
                'cafc_player_id': cafc_player_id,
                'name': playername,
                'data_source': data_source,
                'score': score
            }

    if best_match:
        if best_match['data_source'] == 'internal':
            return {'CAFC_PLAYER_ID': best_match['cafc_player_id']}, None
        else:
            return {'PLAYERID': best_match['playerid']}, None

    return None, f"Player not found: {player_name}"


def find_fixture_unlimited(cursor, fixture_str, fixture_date, fuzzy_log=None):
    """Find fixture with UNLIMITED fuzzy matching (no fixture count limits)."""
    if not fixture_str or pd.isna(fixture_str):
        return None, "Empty fixture"

    if not fixture_date or pd.isna(fixture_date):
        return None, "Empty fixture date"

    # Parse teams
    home_team, away_team = parse_fixture(fixture_str)
    if not home_team or not away_team:
        return None, f"Could not parse fixture: {fixture_str}"

    # Convert date
    try:
        if isinstance(fixture_date, str):
            date_obj = datetime.strptime(fixture_date, "%d/%m/%Y")
        else:
            date_obj = fixture_date
        formatted_date = date_obj.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        return None, f"Invalid date format: {fixture_date}"

    # PHASE 1: Try exact LIKE matching with team name variations
    home_variations = normalize_team_name(home_team)
    away_variations = normalize_team_name(away_team)

    for home_var in home_variations:
        for away_var in away_variations:
            query = """
                SELECT ID, CAFC_MATCH_ID, HOMESQUADNAME, AWAYSQUADNAME, DATA_SOURCE
                FROM MATCHES
                WHERE (
                    UPPER(HOMESQUADNAME) LIKE UPPER(%s) OR UPPER(HOMESQUADNAME) LIKE UPPER(%s)
                ) AND (
                    UPPER(AWAYSQUADNAME) LIKE UPPER(%s) OR UPPER(AWAYSQUADNAME) LIKE UPPER(%s)
                ) AND DATE(SCHEDULEDDATE) = %s
                LIMIT 1
            """

            home_like = f"%{home_var}%"
            away_like = f"%{away_var}%"

            cursor.execute(query, (home_like, away_like, away_like, home_like, formatted_date))
            result = cursor.fetchone()

            if result:
                match_id = result[1] if result[4] == 'internal' else result[0]
                return match_id, None

    # PHASE 2: Fuzzy matching with NO LIMITS
    query = """
        SELECT ID, CAFC_MATCH_ID, HOMESQUADNAME, AWAYSQUADNAME, DATA_SOURCE
        FROM MATCHES
        WHERE DATE(SCHEDULEDDATE) = %s
    """
    cursor.execute(query, (formatted_date,))
    all_fixtures = cursor.fetchall()

    if not all_fixtures:
        return None, f"No fixtures found on {formatted_date}"

    # Normalize search teams
    home_normalized = normalize_unicode(home_team).upper().strip()
    away_normalized = normalize_unicode(away_team).upper().strip()

    best_match = None
    best_score = 0.0

    for fixture in all_fixtures:
        db_home = normalize_unicode(fixture[2]).upper().strip()
        db_away = normalize_unicode(fixture[3]).upper().strip()

        # Calculate similarity scores (both directions)
        home_score = SequenceMatcher(None, home_normalized, db_home).ratio()
        away_score = SequenceMatcher(None, away_normalized, db_away).ratio()
        avg_score = (home_score + away_score) / 2

        # Also try swapped (home/away reversed)
        home_score_swap = SequenceMatcher(None, home_normalized, db_away).ratio()
        away_score_swap = SequenceMatcher(None, away_normalized, db_home).ratio()
        avg_score_swap = (home_score_swap + away_score_swap) / 2

        final_score = max(avg_score, avg_score_swap)

        if final_score > best_score and final_score >= FUZZY_THRESHOLD:
            best_score = final_score
            best_match = fixture

    if best_match:
        if fuzzy_log is not None:
            fuzzy_log.append({
                'search': f"{home_team} vs {away_team}",
                'matched': f"{best_match[2]} vs {best_match[3]}",
                'date': formatted_date,
                'similarity': f"{best_score:.2%}",
                'fixture_count': len(all_fixtures)
            })
        match_id = best_match[1] if best_match[4] == 'internal' else best_match[0]
        return match_id, None

    return None, f"Fixture not found: {fixture_str} on {formatted_date} ({len(all_fixtures)} fixtures checked)"


def analyze_reports(conn, reports_df, player_mappings):
    """Analyze which reports can be imported."""
    print("\n" + "="*80)
    print("ANALYZING ALL REPORTS")
    print("="*80 + "\n")

    cursor = conn.cursor()

    # Load all players and scouts
    print("Loading players from database...")
    cursor.execute("SELECT PLAYERID, CAFC_PLAYER_ID, PLAYERNAME, DATA_SOURCE FROM PLAYERS")
    all_players = cursor.fetchall()
    print(f"‚úì Loaded {len(all_players)} players\n")

    results = {
        'total': 0,
        'can_import': [],
        'cannot_import': [],
        'missing_players': set(),
        'missing_fixtures': set(),
        'fuzzy_matches': [],
        'manual_mappings_used': set()
    }

    # Detect column names
    print("Detecting column names...")
    col_map = {}
    for col in reports_df.columns:
        col_lower = col.lower().strip()
        if col_lower == 'player' or ('player' in col_lower and 'name' in col_lower):
            col_map['player'] = col
        elif col_lower == 'fixture' or (col_lower == 'match' and 'id' not in col_lower):
            if 'fixture' not in col_map:
                col_map['fixture'] = col
        elif col_lower == 'scout' or col_lower == 'author':
            col_map['scout'] = col
        elif col_lower == 'fixture date' or (('fixture' in col_lower or 'match' in col_lower) and 'date' in col_lower):
            col_map['fixture_date'] = col
        elif col_lower == 'grade':
            col_map['grade'] = col

    print(f"‚úì Detected columns: {col_map}\n")

    if not all(k in col_map for k in ['player', 'fixture']):
        print("‚úó ERROR: Could not detect required columns (player, fixture)")
        return results

    print(f"Analyzing {len(reports_df)} reports...\n")
    print(f"{'Row':<6} {'Player':<25} {'Fixture':<30} {'Status':<10}")
    print("-"*80)

    for idx, row in reports_df.iterrows():
        results['total'] += 1

        if results['total'] % 100 == 0:
            can_count = len(results['can_import'])
            cannot_count = len(results['cannot_import'])
            print(f"{results['total']:>6} | Progress: {can_count} can import, {cannot_count} cannot")

        player_name = row.get(col_map.get('player'))
        fixture_str = row.get(col_map.get('fixture'))
        scout_name = row.get(col_map.get('scout'))
        fixture_date = row.get(col_map.get('fixture_date'))

        # Skip reports with empty player names
        if pd.isna(player_name) or not str(player_name).strip():
            continue

        report_info = {
            'row': idx + 1,
            'player_name': player_name,
            'fixture': fixture_str,
            'scout': scout_name,
            'errors': []
        }

        # Check player (with manual mappings)
        player, error = find_player_with_mapping(cursor, player_name, all_players, player_mappings)
        if error:
            report_info['errors'].append(error)
            results['missing_players'].add(str(player_name))
        else:
            if str(player_name) in player_mappings:
                results['manual_mappings_used'].add(str(player_name))

        # Check fixture (unlimited fuzzy matching)
        match_id, error = find_fixture_unlimited(cursor, fixture_str, fixture_date, results['fuzzy_matches'])
        if error:
            report_info['errors'].append(error)
            results['missing_fixtures'].add(f"{fixture_str} on {fixture_date}")

        # Determine if can import (skip scout check as requested)
        if not report_info['errors']:
            results['can_import'].append(report_info)
        else:
            results['cannot_import'].append(report_info)

    cursor.close()

    # Print summary
    print("\n" + "="*80)
    print("ANALYSIS COMPLETE")
    print("="*80 + "\n")

    total = results['total']
    can_count = len(results['can_import'])
    cannot_count = len(results['cannot_import'])

    print(f"üìä SUMMARY")
    print(f"  Total reports analyzed: {total}")
    print(f"  ‚úÖ Can be imported: {can_count} ({can_count/total*100:.1f}%)")
    print(f"  ‚ùå Cannot be imported: {cannot_count} ({cannot_count/total*100:.1f}%)")

    print(f"\nüìã DETAILS")
    print(f"  Missing players: {len(results['missing_players'])}")
    print(f"  Missing fixtures: {len(results['missing_fixtures'])}")
    print(f"  Manual player mappings used: {len(results['manual_mappings_used'])}")
    print(f"  Fuzzy fixture matches: {len(results['fuzzy_matches'])}")

    # Save results
    print(f"\nüíæ SAVING RESULTS TO FILES")

    # Save summary
    with open('final_analysis_summary.txt', 'w') as f:
        f.write("COMPREHENSIVE ANALYSIS SUMMARY\n")
        f.write("="*80 + "\n\n")
        f.write(f"Total reports: {total}\n")
        f.write(f"Can be imported: {can_count} ({can_count/total*100:.1f}%)\n")
        f.write(f"Cannot be imported: {cannot_count} ({cannot_count/total*100:.1f}%)\n\n")
        f.write(f"Missing players: {len(results['missing_players'])}\n")
        f.write(f"Missing fixtures: {len(results['missing_fixtures'])}\n")
        f.write(f"Manual player mappings used: {len(results['manual_mappings_used'])}\n")
        f.write(f"Fuzzy fixture matches: {len(results['fuzzy_matches'])}\n")
    print(f"  ‚úì Saved final_analysis_summary.txt")

    # Save can_import list
    if results['can_import']:
        with open('can_import.txt', 'w') as f:
            f.write("REPORTS THAT CAN BE IMPORTED\n")
            f.write("="*80 + "\n\n")
            f.write(f"Total: {len(results['can_import'])} reports\n\n")
            f.write("="*80 + "\n\n")
            for report in results['can_import']:
                f.write(f"Row {report['row']}: {report['player_name']} - {report['fixture']}\n")
        print(f"  ‚úì Saved can_import.txt ({len(results['can_import'])} reports)")

    # Save cannot_import details
    if results['cannot_import']:
        with open('cannot_import.txt', 'w') as f:
            f.write("REPORTS THAT CANNOT BE IMPORTED\n")
            f.write("="*80 + "\n\n")
            f.write(f"Total: {len(results['cannot_import'])} reports\n\n")
            f.write("="*80 + "\n\n")
            for report in results['cannot_import']:
                f.write(f"Row {report['row']}: {report['player_name']} - {report['fixture']}\n")
                f.write(f"  Scout: {report['scout']}\n")
                f.write(f"  Errors:\n")
                for error in report['errors']:
                    f.write(f"    - {error}\n")
                f.write("\n")
        print(f"  ‚úì Saved cannot_import.txt ({len(results['cannot_import'])} reports)")

    # Save missing players
    if results['missing_players']:
        with open('still_missing_players.txt', 'w') as f:
            f.write("PLAYERS STILL MISSING AFTER MANUAL MAPPINGS\n")
            f.write("="*80 + "\n\n")
            for player in sorted(results['missing_players']):
                f.write(f"{player}\n")
        print(f"  ‚úì Saved still_missing_players.txt ({len(results['missing_players'])} players)")

    # Save missing fixtures
    if results['missing_fixtures']:
        with open('still_missing_fixtures.txt', 'w') as f:
            f.write("FIXTURES STILL MISSING AFTER UNLIMITED FUZZY MATCHING\n")
            f.write("="*80 + "\n\n")
            for fixture in sorted(results['missing_fixtures']):
                f.write(f"{fixture}\n")
        print(f"  ‚úì Saved still_missing_fixtures.txt ({len(results['missing_fixtures'])} fixtures)")

    # Save fuzzy matches log
    if results['fuzzy_matches']:
        with open('fuzzy_matches_used.txt', 'w') as f:
            f.write("FUZZY FIXTURE MATCHES\n")
            f.write("="*80 + "\n\n")
            f.write(f"Total: {len(results['fuzzy_matches'])} fixtures matched via fuzzy logic\n")
            f.write("Similarity threshold: 85%\n\n")
            f.write("="*80 + "\n\n")
            for match in results['fuzzy_matches']:
                f.write(f"SEARCHED FOR: {match['search']}\n")
                f.write(f"MATCHED WITH: {match['matched']}\n")
                f.write(f"DATE: {match['date']}\n")
                f.write(f"SIMILARITY: {match['similarity']}\n")
                f.write(f"FIXTURES ON DATE: {match['fixture_count']}\n")
                f.write("-" * 80 + "\n\n")
        print(f"  ‚úì Saved fuzzy_matches_used.txt ({len(results['fuzzy_matches'])} fuzzy matches)")

    print(f"\n‚úì Analysis complete!")

    return results


def main():
    """Main function."""
    print("="*80)
    print("UNIFIED COMPREHENSIVE ANALYSIS")
    print("="*80 + "\n")

    try:
        # Load player mappings
        player_mappings = load_player_mappings()

        # Read Excel file
        excel_file = Path(EXCEL_FILE)
        if not excel_file.exists():
            print(f"‚úó File not found: {EXCEL_FILE}")
            return

        print(f"Reading: {excel_file.name}")
        df = pd.read_excel(excel_file, sheet_name=0)
        print(f"‚úì Loaded {len(df)} reports\n")

        # Connect to database
        conn = get_snowflake_connection()

        # Analyze reports
        results = analyze_reports(conn, df, player_mappings)

        conn.close()

    except Exception as e:
        print(f"\n‚úó ERROR: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
